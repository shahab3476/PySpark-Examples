{"cells":[{"cell_type":"markdown","source":["The dataset contains six features (first six cloumns) and a target variable (the 7th column) \n\nFirst step is to load the dataset to RDD."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# load the text file to rdd\n# The file is located at 'https://www.dropbox.com/s/ynz19sxmjny7fdi/solved_heuristic_1.txt?dl=0'\nrdd = sc.textFile('/FileStore/tables/bbtax3gn1457669500982/solved_heuristic_2.txt')\nprint rdd.count()\nprint rdd.first()\n\n#  separate the label from the features\nrdd1 = rdd.map(lambda l: l.split(\",\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# convert the RDD to data frame\nschema = StructType([\n    StructField('f1', StringType(), True),\n    StructField('f2', StringType(), True),\n    StructField('f3', StringType(), True),\n    StructField('f4', StringType(), True),\n    StructField('f5', StringType(), True),\n    StructField('f6', StringType(), True),\n    StructField('label', StringType(), True)\n  ])\ndf = sqlContext.createDataFrame(rdd1, schema)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#convert the character columns to numeric\ndf = df.withColumn(\"f1\", df[\"f1\"].cast(IntegerType()))\ndf = df.withColumn(\"f2\", df[\"f2\"].cast(IntegerType()))\ndf = df.withColumn(\"f3\", df[\"f3\"].cast(IntegerType()))\ndf = df.withColumn(\"f4\", df[\"f4\"].cast(IntegerType()))\ndf = df.withColumn(\"f5\", df[\"f5\"].cast(IntegerType()))\ndf = df.withColumn(\"f6\", df[\"f6\"].cast(IntegerType()))\ndf = df.withColumn(\"label\", df[\"label\"].cast(DoubleType()))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df\nprint df.dtypes\ndf.describe().show()\ndf.first()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#use a vector assembler to create features\nfrom pyspark.ml.feature import VectorAssembler\n\nignore = ['label']\nVA = VectorAssembler(\n    inputCols=[x for x in df.columns if x not in ignore],\n    outputCol='features')\n\nVA1 = VA.transform(df)\n#print(VA1.select(\"features\", \"target\").first())\n\ndf1 = (VA1.select(\"features\", \"label\"))\ndf1.first()\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# divide the data to training and testing\nsplits = df1.randomSplit([0.8, 0.2], 2569)\ntrain = splits[0].take(5)\ntest = splits[1].take(5)\n\nprint train.pop(1)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n# create the layers\n# 6 input, 3 intermediate, and 1 output layers\nlayers = [6, 4, 3, 100]\n# create the trainer and set its parameters\n#trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=128, seed=1234)\ntrainer = MultilayerPerceptronClassifier(maxIter=1)\n# train the model\nmodel = trainer.fit(train)\n# compute precision on the test set\nresult = model.transform(test)"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"NeuralNetworks","notebookId":1170390930840850},"nbformat":4,"nbformat_minor":0}
