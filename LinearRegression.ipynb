{"cells":[{"cell_type":"markdown","source":["The dataset contains six features (first six cloumns) and a target variable (the 7th column) \nIt can be downloaded from the drop box location."],"metadata":{}},{"cell_type":"code","source":["# First step is to load the dataset to RDD.\nfrom pyspark.sql.types import *\nimport urllib2\n\n# read the content of the url\ndatafile = urllib2.urlopen('https://www.dropbox.com/s/v4g5fb5ctlf9p4a/solved_heuristic_2.txt?raw=1')\n# get the lines into a vector\nlines = [line.rstrip('\\n') for line in datafile]\n\nrdd = sc.parallelize(lines)\nprint rdd.first()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#  separate the label from the features\nrdd1 = rdd.map(lambda l: l.split(\",\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# convert the RDD to data frame\nschema = StructType([\n    StructField('f1', StringType(), True),\n    StructField('f2', StringType(), True),\n    StructField('f3', StringType(), True),\n    StructField('f4', StringType(), True),\n    StructField('f5', StringType(), True),\n    StructField('f6', StringType(), True),\n    StructField('label', StringType(), True)\n  ])\ndf = sqlContext.createDataFrame(rdd1, schema)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#convert the character columns to numeric\ndf = df.withColumn(\"f1\", df[\"f1\"].cast(IntegerType()))\ndf = df.withColumn(\"f2\", df[\"f2\"].cast(IntegerType()))\ndf = df.withColumn(\"f3\", df[\"f3\"].cast(IntegerType()))\ndf = df.withColumn(\"f4\", df[\"f4\"].cast(IntegerType()))\ndf = df.withColumn(\"f5\", df[\"f5\"].cast(IntegerType()))\ndf = df.withColumn(\"f6\", df[\"f6\"].cast(IntegerType()))\ndf = df.withColumn(\"label\", df[\"label\"].cast(DoubleType()))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df\nprint df.dtypes\ndf.describe().show()\ndf.first()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#use a vector assembler to create features\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.mllib.regression import LabeledPoint\n\nignore = ['label']\nVA = VectorAssembler(\n    inputCols=[x for x in df.columns if x not in ignore],\n    outputCol='features')\n\nVA1 = VA.transform(df)\nprint VA1.first()\n\ndf1 = (VA1.select(\"label\",\"features\"))\nprint df1.first()\n\ndf2 = df1.map(lambda (l,f): LabeledPoint(l,f))\nprint df2.first()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# divide the data to training and testing\nsplits = df2.randomSplit([0.8, 0.2], 2569)\ntrain_data = splits[0]\ntest_data = splits[1]\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.mllib.regression import LinearRegressionWithSGD\n\n# Build the model\nprint type(train_data)\n\nmodel = LinearRegressionWithSGD.train(train_data, iterations=10000, step=0.001)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Evaluate the model on training data\nvaluesAndPreds = train_data.map(lambda p: (p.label, model.predict(p.features)))\nMSE = valuesAndPreds.map(lambda (v, p): (v - p)**2).reduce(lambda x, y: x + y) / valuesAndPreds.count()\nprint(\"Mean Squared Error = \" + str(MSE))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Save and load model\nmodel.save(sc, \"tmpModel\")\nsameModel = LinearRegressionModel.load(sc, \"tmpModel\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# use the model on the test data\nPreds = test_data.map(lambda p: (p.label, model.predict(p.features)))\nprint Preds.first()\nMSE_test = Preds.map(lambda (v, p): (v - p)**2).reduce(lambda x, y: x + y) / Preds.count()\nprint(\"Mean Squared Error = \" + str(MSE_test))"],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"LinearRegression","notebookId":1986862070133523},"nbformat":4,"nbformat_minor":0}
